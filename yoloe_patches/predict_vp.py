from ultralytics.models.yolo.detect import DetectionPredictor
from ultralytics.models.yolo.segment import SegmentationPredictor
from ultralytics.data.augment import LetterBox, LoadVisualPrompt
from ultralytics.utils.instance import Instances
import numpy as np
import torch
from copy import deepcopy

from ultralytics.utils.torch_utils import select_device

class YOLOEVPPredictorMixin:
    def setup_model(self, model, verbose=True):
        """Initialize YOLO model with given parameters and set it to evaluation mode."""
        device = select_device(self.args.device, verbose=verbose)
        self.model = model.to(device)

        self.device = device  # update device
        self.model.fp16 = False
        self.args.half = False
        self.model.eval()
        
        self.done_warmup = True
        self.return_vpe = False
        
    def set_return_vpe(self, return_vpe):
        self.return_vpe = return_vpe
    
    def set_prompts(self, prompts):
        self.prompts = deepcopy(prompts)
    
    def load_vp(self, label):
        label["img"] = label["img"].transpose(2, 0, 1)
        load_vp = LoadVisualPrompt(nc=len(label["cls"]), augment=False)
        label = load_vp(label)
        label["img"] = label["img"].transpose(1, 2, 0)
        return label
    
    def process_box_label(self, img, bboxes, cls, letterbox):
        label = dict(
            img=img,
            instances=Instances(bboxes=bboxes.astype(np.float32), 
                                segments=np.zeros((0, 1000, 2), dtype=np.float32), 
                                bbox_format="xyxy", normalized=False),
            cls=torch.tensor(cls).unsqueeze(-1)
        )
        label = letterbox(label)
        instances = label.pop("instances")
        h, w = label["img"].shape[:2]
        instances.normalize(w, h)
        instances.convert_bbox(format="xywh")
        label["bboxes"] = torch.from_numpy(instances.bboxes)
        return self.load_vp(label)
    
    def process_mask_label(self, img, masks, cls, letterbox):
        img = letterbox(image=img)
        masks = np.stack([letterbox(image=mask) for mask in masks])
        masks[masks == 114] = 0
        label = dict(
            img=img,
            masks=masks,
            cls=torch.tensor(cls).unsqueeze(-1)
        )
        return self.load_vp(label)
        
    def create_class_count_mask(self, cls_arrays, unified_nc):
        """åˆ›å»ºç±»åˆ«è®¡æ•°æ©ç ï¼Œè®°å½•æ¯ä¸ªç±»åˆ«åœ¨å¤šå°‘ä¸ªpromptä¸­å‡ºç°
        
        Args:
            cls_arrays: list of np.arrayï¼Œæ¯ä¸ªpromptçš„ç±»åˆ«IDæ•°ç»„
            unified_nc: ç»Ÿä¸€çš„ç±»åˆ«æ•°é‡
            
       pad_and_align_vpe Returns:
            torch.Tensor: [unified_nc] è®¡æ•°æ©ç ï¼Œè®°å½•æ¯ä¸ªç±»åˆ«å‡ºç°çš„æ¬¡æ•°
        """
        class_count_mask = torch.zeros(unified_nc, dtype=torch.float32)
        
        for cls_array in cls_arrays:
            if isinstance(cls_array, np.ndarray) and cls_array.size > 0:
                unique_classes = np.unique(cls_array)
                for class_id in unique_classes:
                    if 0 <= class_id < unified_nc:
                        class_count_mask[class_id] += 1.0
        
        print(f"[Debug] ç±»åˆ«è®¡æ•°æ©ç : {class_count_mask}")
        for i in range(unified_nc):
            if class_count_mask[i] > 0:
                print(f"[Debug] ç±»åˆ«{i}: å‡ºç°åœ¨{int(class_count_mask[i])}ä¸ªpromptä¸­")
                
        return class_count_mask
    
    def combine_vpe_by_class_averaging_correct(self, raw_vpe, class_count_mask):
        """æ­£ç¡®çš„é€šé“çº§åˆ«åˆ°ç±»åˆ«çº§åˆ«VPEè½¬æ¢
        
        åŸºäºå®é™…çš„VPEå½¢çŠ¶ï¼š[batch, num_channels, embed_dim]
        å…¶ä¸­ num_channels = num_prompts * nc
        
        Args:
            raw_vpe: torch.Tensor [batch, num_channels, embed_dim] é€šé“çº§åˆ«çš„VPE
            class_count_mask: torch.Tensor [nc] ç±»åˆ«è®¡æ•°æ©ç 
            
        Returns:
            torch.Tensor: [batch, nc, embed_dim] ç±»åˆ«çº§åˆ«çš„VPE
        """
        batch_size, num_channels, embed_dim = raw_vpe.shape
        
        # è®¡ç®—ç»Ÿä¸€çš„ç±»åˆ«æ•°é‡å’Œpromptæ•°é‡
        nc = len(class_count_mask)
        num_prompts = len(self.prompts_cls)
        expected_channels = num_prompts * nc
        
        print(f"[Debug] é€šé“çº§åˆ«åˆ°ç±»åˆ«çº§åˆ«VPEè½¬æ¢:")
        print(f"  è¾“å…¥VPEå½¢çŠ¶: {raw_vpe.shape}")
        print(f"  é¢„æœŸé€šé“æ•°: {expected_channels} (num_prompts={num_prompts} Ã— nc={nc})")
        print(f"  å®é™…é€šé“æ•°: {num_channels}")
        print(f"  ç±»åˆ«è®¡æ•°: {class_count_mask}")
        
        if num_channels != expected_channels:
            print(f"[Warning] é€šé“æ•°ä¸åŒ¹é…ï¼Œä½¿ç”¨å®é™…é€šé“æ•°è¿›è¡Œå¤„ç†")
            # é‡æ–°è®¡ç®—ncï¼Œå‡è®¾æ‰€æœ‰promptéƒ½æœ‰ç›¸åŒçš„ç±»åˆ«æ•°
            nc = num_channels // num_prompts
            print(f"[Debug] é‡æ–°è®¡ç®—çš„nc: {nc}")
        
        # åˆ›å»ºç±»åˆ«çº§åˆ«çš„VPE
        combined_vpe = torch.zeros(batch_size, nc, embed_dim, 
                                 device=raw_vpe.device, dtype=raw_vpe.dtype)
        
        print(f"[Debug] é€šé“åˆ°ç±»åˆ«æ˜ å°„:")
        
        # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œå¤„ç†
        for class_id in range(nc):
            # æ‰¾åˆ°è¯¥ç±»åˆ«å¯¹åº”çš„æ‰€æœ‰é€šé“
            class_channels = []
            class_weights = []
            
            for prompt_idx in range(num_prompts):
                channel_idx = prompt_idx * nc + class_id
                
                if channel_idx < num_channels:
                    # æ£€æŸ¥è¯¥promptæ˜¯å¦åŒ…å«è¿™ä¸ªç±»åˆ«
                    if prompt_idx < len(self.prompts_cls):
                        cls_array = self.prompts_cls[prompt_idx]
                        if isinstance(cls_array, np.ndarray) and class_id in cls_array:
                            class_channels.append(channel_idx)
                            class_weights.append(1.0)
                            print(f"    ç±»åˆ«{class_id}: é€šé“{channel_idx} (æ¥è‡ªPrompt {prompt_idx}) âœ“")
                        else:
                            print(f"    ç±»åˆ«{class_id}: é€šé“{channel_idx} (æ¥è‡ªPrompt {prompt_idx}) âœ— (ä¸åŒ…å«è¯¥ç±»åˆ«)")
                    else:
                        # å¦‚æœæ²¡æœ‰clsä¿¡æ¯ï¼Œå‡è®¾æ‰€æœ‰é€šé“éƒ½æœ‰æ•ˆ
                        class_channels.append(channel_idx)
                        class_weights.append(1.0)
                        print(f"    ç±»åˆ«{class_id}: é€šé“{channel_idx} (æ¥è‡ªPrompt {prompt_idx}) ? (æ— clsä¿¡æ¯)")
            
            # å¯¹è¯¥ç±»åˆ«çš„æ‰€æœ‰æœ‰æ•ˆé€šé“è¿›è¡ŒåŠ æƒå¹³å‡
            if class_channels:
                class_weights = torch.tensor(class_weights, device=raw_vpe.device, dtype=raw_vpe.dtype)
                class_weights = class_weights / class_weights.sum()  # å½’ä¸€åŒ–
                
                class_vpe = torch.zeros(batch_size, embed_dim, device=raw_vpe.device, dtype=raw_vpe.dtype)
                for i, channel_idx in enumerate(class_channels):
                    class_vpe += raw_vpe[:, channel_idx, :] * class_weights[i]
                
                combined_vpe[:, class_id, :] = class_vpe
                
                print(f"    ç±»åˆ«{class_id}: ä½¿ç”¨{len(class_channels)}ä¸ªé€šé“è¿›è¡Œå¹³å‡")
                print(f"      é€šé“ç´¢å¼•: {class_channels}")
                print(f"      æƒé‡: {class_weights.tolist()}")
                print(f"      VPEèŒƒæ•°: {torch.norm(class_vpe).item():.4f}")
            else:
                print(f"    ç±»åˆ«{class_id}: æ— æœ‰æ•ˆé€šé“ï¼Œä¿æŒé›¶å€¼")
        
        print(f"[Debug] ç±»åˆ«çº§åˆ«VPEè½¬æ¢å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {combined_vpe.shape}")
        return combined_vpe
    
    def combine_vpe_overall_averaging(self, raw_vpe, class_count_mask):
        """æ•´ä½“å¹³å‡æ–¹æ¡ˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        å½“VPEç»´åº¦ä¸èƒ½æŒ‰ç±»åˆ«åˆ†å‰²æ—¶ä½¿ç”¨
        """
        batch_size, num_prompts, embed_dim = raw_vpe.shape
        
        print(f"[Debug] ä½¿ç”¨æ•´ä½“å¹³å‡æ–¹æ¡ˆ: {raw_vpe.shape} -> [batch, 1, {embed_dim}]")
        
        # è®¡ç®—æ¯ä¸ªpromptçš„æƒé‡ï¼ˆåŸºäºå®ƒåŒ…å«çš„æœ‰æ•ˆç±»åˆ«æ•°é‡ï¼‰
        prompt_weights = []
        for prompt_idx, cls_array in enumerate(self.prompts_cls):
            if isinstance(cls_array, np.ndarray) and cls_array.size > 0:
                # æƒé‡ = è¯¥promptåŒ…å«çš„æœ‰æ•ˆç±»åˆ«æ•°é‡
                valid_classes = [c for c in np.unique(cls_array) if class_count_mask[c] > 0]
                weight = len(valid_classes)
                prompt_weights.append(weight)
                print(f"[Debug] Prompt {prompt_idx}: åŒ…å«{weight}ä¸ªæœ‰æ•ˆç±»åˆ«")
            else:
                prompt_weights.append(0.0)
                print(f"[Debug] Prompt {prompt_idx}: æ— æœ‰æ•ˆç±»åˆ«")
        
        # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
        prompt_weights = torch.tensor(prompt_weights, device=raw_vpe.device, dtype=raw_vpe.dtype)
        if prompt_weights.sum() > 0:
            prompt_weights = prompt_weights / prompt_weights.sum()
        else:
            prompt_weights = torch.ones(num_prompts, device=raw_vpe.device) / num_prompts
        
        print(f"[Debug] å½’ä¸€åŒ–æƒé‡: {prompt_weights}")
        
        # åŠ æƒå¹³å‡
        combined_vpe = torch.zeros(batch_size, 1, embed_dim, device=raw_vpe.device, dtype=raw_vpe.dtype)
        for prompt_idx in range(num_prompts):
            combined_vpe[:, 0, :] += raw_vpe[:, prompt_idx, :] * prompt_weights[prompt_idx]
        
        # å½’ä¸€åŒ–
        combined_vpe = torch.nn.functional.normalize(combined_vpe, dim=-1, p=2)
        
        print(f"[Debug] æ•´ä½“å¹³å‡VPEç»“åˆå®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {combined_vpe.shape}")
        return combined_vpe

    def pre_transform(self, im):
        letterbox = LetterBox(
            self.imgsz,
            auto=False,
            stride=int(self.model.stride[-1].item()),
        )

        cls = self.prompts["cls"]
        cls = [cls] if not isinstance(cls, list) else cls
        
        # ğŸš€ ä¿å­˜åŸå§‹clsä¿¡æ¯ï¼Œç”¨äºåç»­VPEç»“åˆ
        self.prompts_cls = cls
        
        # ğŸ”§ æ–°å¢ï¼šç»Ÿä¸€ç±»åˆ«ç©ºé—´å¤„ç†
        # 1. æ‰¾åˆ°æ‰€æœ‰promptä¸­çš„æœ€å¤§ç±»åˆ«IDï¼Œç¡®å®šç»Ÿä¸€çš„ç±»åˆ«ç©ºé—´å¤§å°
        all_class_ids = set()
        for cls_array in cls:
            if isinstance(cls_array, np.ndarray) and cls_array.size > 0:
                all_class_ids.update(cls_array.tolist())
        
        if not all_class_ids:
            raise ValueError("No valid class IDs found in prompts")
        
        max_class_id = max(all_class_ids)
        unified_nc = max_class_id + 1  # ç»Ÿä¸€çš„ç±»åˆ«æ•°é‡
        
        print(f"[Debug] ç»Ÿä¸€ç±»åˆ«ç©ºé—´: å‘ç°ç±»åˆ«ID {sorted(all_class_ids)}, ç»Ÿä¸€nc={unified_nc}")
        
        # ğŸš€ æ–°å¢ï¼šåˆ›å»ºç±»åˆ«è®¡æ•°æ©ç ï¼Œç”¨äºåç»­VPEç»“åˆ
        self.class_count_mask = self.create_class_count_mask(cls, unified_nc)
        self.unified_nc = unified_nc
            
        if "bboxes" in self.prompts:
            bboxes = self.prompts["bboxes"]
            bboxes = [bboxes] if not isinstance(bboxes, list) else bboxes
            
            # ğŸ”§ ä¿®æ”¹ï¼šä¸ºæ¯ä¸ªpromptå•ç‹¬å¤„ç†ï¼Œä¿æŒç‹¬ç«‹
            labels = []
            self.individual_prompts = []  # ä¿å­˜æ¯ä¸ªpromptçš„visual prompts
            self.prompt_unique_cls = []   # ğŸ”§ æ–°å¢ï¼šä¿å­˜æ¯ä¸ªpromptçš„uniqueç±»åˆ«
            for i in range(len(im)):
                label = self.process_box_label_individual(im[i], bboxes[i], cls[i], letterbox)
                labels.append(label)
                self.individual_prompts.append(label["visuals"])
                self.prompt_unique_cls.append(label["unique_cls"])  # ğŸ”§ ä¿å­˜uniqueç±»åˆ«
                
        elif "masks" in self.prompts:
            masks = self.prompts["masks"]
            masks = [masks] if not isinstance(masks, list) else masks
            
            # ğŸ”§ ä¿®æ”¹ï¼šä¸ºæ¯ä¸ªpromptå•ç‹¬å¤„ç†ï¼Œä¿æŒç‹¬ç«‹
            labels = []
            self.individual_prompts = []  # ä¿å­˜æ¯ä¸ªpromptçš„visual prompts
            self.prompt_unique_cls = []   # ğŸ”§ æ–°å¢ï¼šä¿å­˜æ¯ä¸ªpromptçš„uniqueç±»åˆ«
            for i in range(len(im)):
                label = self.process_mask_label_individual(im[i], masks[i], cls[i], letterbox)
                labels.append(label)
                self.individual_prompts.append(label["visuals"])
                self.prompt_unique_cls.append(label["unique_cls"])  # ğŸ”§ ä¿å­˜uniqueç±»åˆ«
        else:
            raise ValueError("Please provide valid bboxes or masks")

        # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä¸åˆå¹¶promptsï¼Œä¿æŒç‹¬ç«‹å¤„ç†
        print(f"[Debug] ä¿å­˜{len(self.individual_prompts)}ä¸ªç‹¬ç«‹çš„visual prompts")
        for i, (prompt, unique_cls) in enumerate(zip(self.individual_prompts, self.prompt_unique_cls)):
            print(f"  Prompt {i}: {prompt.shape}, uniqueç±»åˆ«: {unique_cls}")
        
        # ä½¿ç”¨ç»Ÿä¸€çš„ç±»åˆ«æ•°é‡
        self.model.model[-1].nc = unified_nc
        self.model.names = [f"object{i}" for i in range(unified_nc)]
        
        return [label["img"] for label in labels]
    
    def process_box_label_individual(self, img, bboxes, cls, letterbox):
        """å¤„ç†å•ä¸ªpromptçš„bboxæ ‡ç­¾ï¼Œä½¿ç”¨uniqueç±»åˆ«æ•°é‡"""
        # ğŸ”§ å…³é”®ä¿®æ­£ï¼šè®¡ç®—uniqueç±»åˆ«æ•°é‡
        unique_cls = np.unique(cls)
        unique_nc = len(unique_cls)
        
        print(f"[Debug] Promptå¤„ç†: {len(cls)}ä¸ªobject, {len(unique_cls)}ä¸ªuniqueç±»åˆ«")
        print(f"[Debug] Objectç±»åˆ«: {cls}")
        print(f"[Debug] Uniqueç±»åˆ«: {unique_cls}")
        
        label = dict(
            img=img,
            instances=Instances(bboxes=bboxes.astype(np.float32), 
                                segments=np.zeros((0, 1000, 2), dtype=np.float32), 
                                bbox_format="xyxy", normalized=False),
            cls=torch.tensor(cls).unsqueeze(-1)
        )
        label = letterbox(label)
        instances = label.pop("instances")
        h, w = label["img"].shape[:2]
        instances.normalize(w, h)
        instances.convert_bbox(format="xywh")
        label["bboxes"] = torch.from_numpy(instances.bboxes)
        
        # ğŸ”§ ä¼ é€’uniqueç±»åˆ«ä¿¡æ¯
        return self.load_vp_individual(label, unique_nc, unique_cls)
    
    def process_mask_label_individual(self, img, masks, cls, letterbox):
        """å¤„ç†å•ä¸ªpromptçš„maskæ ‡ç­¾ï¼Œä½¿ç”¨uniqueç±»åˆ«æ•°é‡"""
        # ğŸ”§ å…³é”®ä¿®æ­£ï¼šè®¡ç®—uniqueç±»åˆ«æ•°é‡
        unique_cls = np.unique(cls)
        unique_nc = len(unique_cls)
        
        print(f"[Debug] Promptå¤„ç†: {len(cls)}ä¸ªobject, {len(unique_cls)}ä¸ªuniqueç±»åˆ«")
        print(f"[Debug] Objectç±»åˆ«: {cls}")
        print(f"[Debug] Uniqueç±»åˆ«: {unique_cls}")
        
        img = letterbox(image=img)
        masks = np.stack([letterbox(image=mask) for mask in masks])
        masks[masks == 114] = 0
        label = dict(
            img=img,
            masks=masks,
            cls=torch.tensor(cls).unsqueeze(-1)
        )
        
        # ğŸ”§ ä¼ é€’uniqueç±»åˆ«ä¿¡æ¯
        return self.load_vp_individual(label, unique_nc, unique_cls)
        
    def load_vp_individual(self, label, unique_nc, unique_cls):
        """ä½¿ç”¨uniqueç±»åˆ«æ•°é‡åŠ è½½visual prompt"""
        print(f"[Debug] LoadVisualPrompt: unique_nc={unique_nc}, uniqueç±»åˆ«={unique_cls}")
        print(f"[Debug] åŸå§‹ç±»åˆ«æ•°ç»„é•¿åº¦={len(label['cls'])}, å†…å®¹={label['cls'].flatten()}")
        
        label["img"] = label["img"].transpose(2, 0, 1)
        # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨uniqueç±»åˆ«æ•°é‡
        load_vp = LoadVisualPrompt(nc=unique_nc, augment=False)
        label = load_vp(label)
        label["img"] = label["img"].transpose(1, 2, 0)
        
        print(f"[Debug] LoadVisualPromptè¾“å‡ºå½¢çŠ¶: {label['visuals'].shape}")
        
        # ğŸ”§ ä¿å­˜uniqueç±»åˆ«ä¿¡æ¯ï¼Œç”¨äºåç»­å¡«å……
        label["unique_cls"] = unique_cls
        return label

    def get_individual_vpe(self, im, prompt_visual):
        """è·å–å•ä¸ªpromptçš„VPE"""
        # å°†å•ä¸ªpromptè½¬æ¢ä¸ºbatchæ ¼å¼
        prompt_batch = prompt_visual.unsqueeze(0).to(self.device)  # [1, nc, H, W]
        
        # è·å–VPE
        vpe = self.model.get_visual_pe(im, visual=prompt_batch)  # [1, nc, 512]
        return vpe

    def pad_and_align_vpe(self, vpe, unique_cls, unified_nc):
        """Fixed version with proper class alignment"""
        batch_size, vpe_nc, embed_dim = vpe.shape
        
        # Create aligned VPE tensor
        aligned_vpe = torch.zeros(batch_size, unified_nc, embed_dim, 
                                device=vpe.device, dtype=vpe.dtype)
        
        # Handle case where we have more VPE channels than unique classes
        num_classes_to_map = min(vpe_nc, len(unique_cls))
        
        for vpe_idx in range(num_classes_to_map):
            if vpe_idx < len(unique_cls):
                class_id = unique_cls[vpe_idx]
                if 0 <= class_id < unified_nc:
                    aligned_vpe[:, class_id, :] = vpe[:, vpe_idx, :]
        
        return aligned_vpe

    def combine_vpe_by_class_averaging_correct(self, individual_vpes, class_count_mask):
        """æ­£ç¡®çš„ç±»åˆ«çº§åˆ«VPEåŠ æƒå¹³å‡
        
        Args:
            individual_vpes: List[torch.Tensor] æ¯ä¸ªpromptçš„VPEï¼Œå·²å¯¹é½åˆ°ç»Ÿä¸€ç±»åˆ«ç©ºé—´
            class_count_mask: torch.Tensor [nc] ç±»åˆ«è®¡æ•°æ©ç 
            
        Returns:
            torch.Tensor: [batch, nc, embed_dim] åŠ æƒå¹³å‡åçš„VPE
        """
        if not individual_vpes:
            raise ValueError("No individual VPEs provided")
        
        batch_size, nc, embed_dim = individual_vpes[0].shape
        
        print(f"[Debug] ç±»åˆ«çº§åˆ«VPEåŠ æƒå¹³å‡:")
        print(f"  è¾“å…¥: {len(individual_vpes)}ä¸ªVPEï¼Œæ¯ä¸ªå½¢çŠ¶: {individual_vpes[0].shape}")
        print(f"  ç±»åˆ«è®¡æ•°: {class_count_mask}")
        
        # åˆ›å»ºç»“æœVPE
        combined_vpe = torch.zeros(batch_size, nc, embed_dim, 
                                 device=individual_vpes[0].device, 
                                 dtype=individual_vpes[0].dtype)
        
        # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡ŒåŠ æƒå¹³å‡
        for class_id in range(nc):
            class_vpes = []
            class_weights = []
            
            # æ”¶é›†è¯¥ç±»åˆ«åœ¨å„ä¸ªpromptä¸­çš„VPE
            for prompt_idx, vpe in enumerate(individual_vpes):
                cls_array = self.prompts_cls[prompt_idx]
                
                # æ£€æŸ¥è¯¥promptæ˜¯å¦åŒ…å«è¿™ä¸ªç±»åˆ«
                if isinstance(cls_array, np.ndarray) and class_id in cls_array:
                    class_vpes.append(vpe[:, class_id, :])  # [batch, embed_dim]
                    class_weights.append(1.0)
                    print(f"    ç±»åˆ«{class_id}: æ¥è‡ªPrompt {prompt_idx} âœ“")
            
            # è¿›è¡ŒåŠ æƒå¹³å‡
            if class_vpes:
                # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–æƒé‡
                class_weights = torch.tensor(class_weights, 
                                           device=individual_vpes[0].device, 
                                           dtype=individual_vpes[0].dtype)
                class_weights = class_weights / class_weights.sum()
                
                # åŠ æƒå¹³å‡
                class_vpe_avg = torch.zeros_like(class_vpes[0])
                for i, class_vpe in enumerate(class_vpes):
                    class_vpe_avg += class_vpe * class_weights[i]
                
                combined_vpe[:, class_id, :] = class_vpe_avg
                
                print(f"    ç±»åˆ«{class_id}: ä½¿ç”¨{len(class_vpes)}ä¸ªVPEè¿›è¡ŒåŠ æƒå¹³å‡")
                print(f"      æƒé‡: {class_weights.tolist()}")
                print(f"      VPEèŒƒæ•°: {torch.norm(class_vpe_avg).item():.4f}")
            else:
                print(f"    ç±»åˆ«{class_id}: æ— VPEï¼Œä¿æŒé›¶å€¼")
        
        print(f"[Debug] ç±»åˆ«çº§åˆ«VPEåŠ æƒå¹³å‡å®Œæˆï¼Œè¾“å‡ºå½¢çŠ¶: {combined_vpe.shape}")
        return combined_vpe

    def inference(self, im, *args, **kwargs):
        if self.return_vpe:
            print(f"[Debug] å¼€å§‹å•ç‹¬å¤„ç†æ¯ä¸ªpromptçš„VPE")
            
            # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šå•ç‹¬å¤„ç†æ¯ä¸ªprompt
            individual_vpes = []
            
            for prompt_idx, (prompt_visual, unique_cls) in enumerate(zip(self.individual_prompts, self.prompt_unique_cls)):
                print(f"[Debug] å¤„ç†Prompt {prompt_idx}, å½¢çŠ¶: {prompt_visual.shape}, uniqueç±»åˆ«: {unique_cls}")
                
                # è·å–å•ä¸ªpromptçš„VPE
                vpe = self.get_individual_vpe(im, prompt_visual)  # [1, unique_nc, 512]
                print(f"[Debug] Prompt {prompt_idx} VPEå½¢çŠ¶: {vpe.shape}")
                
                # å¡«å……åˆ°ç»Ÿä¸€ç±»åˆ«ç©ºé—´
                aligned_vpe = self.pad_and_align_vpe(vpe, unique_cls, self.unified_nc)
                print(f"[Debug] Prompt {prompt_idx} å¯¹é½åVPEå½¢çŠ¶: {aligned_vpe.shape}")
                
                individual_vpes.append(aligned_vpe)
            
            # ğŸ¯ æŒ‰ç±»åˆ«è¿›è¡ŒåŠ æƒå¹³å‡
            self.vpe = self.combine_vpe_by_class_averaging_correct(individual_vpes, self.class_count_mask)
            print(f"[Debug] æœ€ç»ˆVPEå½¢çŠ¶: {self.vpe.shape}")
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥åˆå¹¶åçš„visual promptsç”¨äºå®é™…æ¨ç†
        # ä¸´æ—¶åˆå¹¶promptsç”¨äºæ¨ç†
        merged_prompts = torch.nn.utils.rnn.pad_sequence(self.individual_prompts, batch_first=True).to(self.device)
        return super().inference(im, vpe=merged_prompts, *args, **kwargs)

class YOLOEVPDetectPredictor(YOLOEVPPredictorMixin, DetectionPredictor):
    pass

class YOLOEVPSegPredictor(YOLOEVPPredictorMixin, SegmentationPredictor):
    pass